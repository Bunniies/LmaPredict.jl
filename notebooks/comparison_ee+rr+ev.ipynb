{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling ScikitLearn [3646fa90-6ef7-5e7e-9f22-8aca16db6324]\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mModule StatsBase with build ID fafbfcfd-d044-e7f2-0002-6fc13fc7eaf9 is missing from the cache.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mThis may mean StatsBase [2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91] does not support precompilation but is imported by a module that does.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:1948\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSkipping precompilation since __precompile__(false). Importing ScikitLearn [3646fa90-6ef7-5e7e-9f22-8aca16db6324].\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling DataFrames [a93c6f00-e57d-5684-b7b6-d8193f3e46c0]\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mModule Missings with build ID fafbfcfd-b2ef-bec6-0002-6fbf642e491f is missing from the cache.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mThis may mean Missings [e1d29d7a-bbdc-5cf2-9ac0-f12de2c33e28] does not support precompilation but is imported by a module that does.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:1948\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSkipping precompilation since __precompile__(false). Importing DataFrames [a93c6f00-e57d-5684-b7b6-d8193f3e46c0].\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling InlineStrings [842dd82b-1e85-43dc-bf29-5d0ee9dffc48]\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m BangBangDataFramesExt\n",
      "\u001b[33m  ✓ \u001b[39mDataFrames\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBangBang → BangBangDataFramesExt\u001b[39m\n",
      "  2 dependencies successfully precompiled in 12 seconds. 42 already precompiled.\n",
      "  \u001b[33m1\u001b[39m dependency precompiled but a different version is currently loaded. Restart julia to access the new version\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling BangBangDataFramesExt [d787bcad-b5c5-56bb-adaa-6bfddb178a59]\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mModule BangBang with build ID fafbfcfd-63c1-e7dd-0002-6fc129d19fd8 is missing from the cache.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mThis may mean BangBang [198e06fe-97b7-11e9-32a5-e1d131e6ad66] does not support precompilation but is imported by a module that does.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:1948\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSkipping precompilation since __precompile__(false). Importing BangBangDataFramesExt [d787bcad-b5c5-56bb-adaa-6bfddb178a59].\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m TransducersDataFramesExt\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mTransducers → TransducersDataFramesExt\u001b[39m\n",
      "  1 dependency successfully precompiled in 1 seconds. 55 already precompiled.\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling TransducersDataFramesExt [cefb4096-3352-5e5f-8501-71f024082a88]\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mModule Transducers with build ID fafbfcfd-b640-9362-0002-6fc5f0b990a8 is missing from the cache.\n",
      "\u001b[33m\u001b[1m│ \u001b[22m\u001b[39mThis may mean Transducers [28d57a85-8fef-5791-bfe6-a80928e7c999] does not support precompilation but is imported by a module that does.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base loading.jl:1948\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSkipping precompilation since __precompile__(false). Importing TransducersDataFramesExt [cefb4096-3352-5e5f-8501-71f024082a88].\n"
     ]
    }
   ],
   "source": [
    "using LmaPredict, Flux, Statistics, StatsBase, ScikitLearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"../plots\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const path_config = \"/Users/lukasgeyer/Studium/Computational Sciences/Masterarbeit/Daten Simon/dat\"\n",
    "const path_plot = \"../plots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = readdir(path_config)[2:5001]\n",
    "idx = sortperm( parse.(Int64, fname))\n",
    "fname = fname[idx]\n",
    "em_n = \"VV\"\n",
    "\n",
    "cnfgarr = Vector{LMAConfig}(undef, 0)\n",
    "for f in fname\n",
    "    push!(cnfgarr, get_LMAConfig(joinpath(path_config, f), \"g5-g5\", em=em_n, bc=false))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data in training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCNFG = length(cnfgarr)\n",
    "train_size = 500\n",
    "test_size = NCNFG - train_size\n",
    "\n",
    "TSRC = \"24\"\n",
    "TVALS = length(cnfgarr[1].data[\"rr\"][TSRC]) - 1\n",
    "if em_n == \"PA\"\n",
    "    EIGVALS = 32\n",
    "else \n",
    "    EIGVALS = 64\n",
    "end\n",
    "\n",
    "eigvals_data_train = Array{Float64}(undef, EIGVALS, train_size)\n",
    "rr_data_train = Array{Float64}(undef, TVALS, train_size)\n",
    "ee_data_train = Array{Float64}(undef, TVALS, train_size)\n",
    "re_data_train = Array{Float64}(undef, TVALS, train_size)\n",
    "\n",
    "eigvals_data_test = Array{Float64}(undef, EIGVALS, test_size)\n",
    "rr_data_test = Array{Float64}(undef, TVALS, test_size)\n",
    "ee_data_test = Array{Float64}(undef, TVALS, test_size)\n",
    "re_data_test = Array{Float64}(undef, TVALS, test_size)\n",
    "\n",
    "for (k, dd) in enumerate(getfield.(cnfgarr, :data)[1:train_size])\n",
    "    eigvals_data_train[:,k] = copy(cnfgarr[k].data[\"eigvals\"][1:EIGVALS])\n",
    "    rr_data_train[:,k] = getindex(getindex(dd, \"rr\"), TSRC)[2:end]\n",
    "    #ee_data_train[:,k] = getindex(getindex(dd, \"ee\"), TSRC)[2:end]\n",
    "    re_data_train[:,k] = getindex(getindex(dd, \"re\"), TSRC)[2:end]\n",
    "\n",
    "    ee_all_TSRC = Matrix{Float64}(undef, TVALS, TVALS)\n",
    "    for ee_TSRC in 0:TVALS-1\n",
    "        ee_all_TSRC[:,ee_TSRC+1] = getindex(getindex(dd, \"ee\"), \"$ee_TSRC\")[2:end]\n",
    "    end\n",
    "    ee_data_train[:,k] = mean(ee_all_TSRC, dims=2)\n",
    "end\n",
    "for (k, dd) in enumerate(getfield.(cnfgarr, :data)[train_size+1:NCNFG])\n",
    "    eigvals_data_test[:,k] = copy(cnfgarr[k].data[\"eigvals\"][1:EIGVALS])\n",
    "    rr_data_test[:,k] = getindex(getindex(dd, \"rr\"), TSRC)[2:end]\n",
    "    #ee_data_test[:,k] = getindex(getindex(dd, \"ee\"), TSRC)[2:end]\n",
    "    re_data_test[:,k] = getindex(getindex(dd, \"re\"), TSRC)[2:end]\n",
    "\n",
    "    ee_all_TSRC = Matrix{Float64}(undef, TVALS, TVALS)\n",
    "    for ee_TSRC in 0:TVALS-1\n",
    "        ee_all_TSRC[:,ee_TSRC+1] = getindex(getindex(dd, \"ee\"), \"$ee_TSRC\")[2:end]\n",
    "    end\n",
    "    ee_data_test[:,k] = mean(ee_all_TSRC, dims=2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×1 Matrix{Int64}:\n",
       " 3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "\n",
    "k_fold_DataSet = Array{Matrix{Float64}}(undef, k)\n",
    "for i in 1:k\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Matrix{Float64}}:\n",
       "    [3.0;;]\n",
       " #undef\n",
       " #undef\n",
       " #undef\n",
       " #undef"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_fold_DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "ArgumentError: invalid index: ([101, 102, 103, 104, 105, 106, 107, 108, 109, 110  …  491, 492, 493, 494, 495, 496, 497, 498, 499, 500], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  91, 92, 93, 94, 95, 96, 97, 98, 99, 100]) of type Tuple{Vector{Int64}, Vector{Int64}}",
     "output_type": "error",
     "traceback": [
      "ArgumentError: invalid index: ([101, 102, 103, 104, 105, 106, 107, 108, 109, 110  …  491, 492, 493, 494, 495, 496, 497, 498, 499, 500], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  91, 92, 93, 94, 95, 96, 97, 98, 99, 100]) of type Tuple{Vector{Int64}, Vector{Int64}}",
      "",
      "Stacktrace:",
      " [1] to_index(i::Tuple{Vector{Int64}, Vector{Int64}})",
      "   @ Base ./indices.jl:300",
      " [2] to_index(A::Matrix{Float64}, i::Tuple{Vector{Int64}, Vector{Int64}})",
      "   @ Base ./indices.jl:277",
      " [3] _to_indices1(A::Matrix{Float64}, inds::Tuple{Base.OneTo{Int64}}, I1::Tuple{Vector{Int64}, Vector{Int64}})",
      "   @ Base ./indices.jl:359",
      " [4] to_indices",
      "   @ ./indices.jl:354 [inlined]",
      " [5] to_indices",
      "   @ ./indices.jl:345 [inlined]",
      " [6] getindex(A::Matrix{Float64}, I::Tuple{Vector{Int64}, Vector{Int64}})",
      "   @ Base ./abstractarray.jl:1291",
      " [7] top-level scope",
      "   @ In[44]:1"
     ]
    }
   ],
   "source": [
    "input_data_train_standardized[CrossValidation.KFold(500, n_folds=5)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Vector{Tuple{Vector{Int64}, Vector{Int64}}}:\n",
       " ([101, 102, 103, 104, 105, 106, 107, 108, 109, 110  …  491, 492, 493, 494, 495, 496, 497, 498, 499, 500], [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  91, 92, 93, 94, 95, 96, 97, 98, 99, 100])\n",
       " ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  491, 492, 493, 494, 495, 496, 497, 498, 499, 500], [101, 102, 103, 104, 105, 106, 107, 108, 109, 110  …  191, 192, 193, 194, 195, 196, 197, 198, 199, 200])\n",
       " ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  491, 492, 493, 494, 495, 496, 497, 498, 499, 500], [201, 202, 203, 204, 205, 206, 207, 208, 209, 210  …  291, 292, 293, 294, 295, 296, 297, 298, 299, 300])\n",
       " ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  491, 492, 493, 494, 495, 496, 497, 498, 499, 500], [301, 302, 303, 304, 305, 306, 307, 308, 309, 310  …  391, 392, 393, 394, 395, 396, 397, 398, 399, 400])\n",
       " ([1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  391, 392, 393, 394, 395, 396, 397, 398, 399, 400], [401, 402, 403, 404, 405, 406, 407, 408, 409, 410  …  491, 492, 493, 494, 495, 496, 497, 498, 499, 500])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CrossValidation.KFold(500, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = 2*TVALS + EIGVALS\n",
    "output_length = TVALS\n",
    "\n",
    "#input_shape_train = vcat(1 ./ eigvals_data_train, ee_data_train, rr_data_train)\n",
    "input_shape_train = vcat(ee_data_train, rr_data_train)\n",
    "output_shape_train = re_data_train\n",
    "\n",
    "#input_shape_test = vcat(1 ./ eigvals_data_test, ee_data_test, rr_data_test)\n",
    "input_shape_test = vcat(ee_data_test, rr_data_test)\n",
    "output_shape_test = re_data_test;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data, normalized and standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_train = maximum(input_shape_train)\n",
    "min_input_train = minimum(input_shape_train)\n",
    "\n",
    "mean_input_train = mean(input_shape_train, dims=ndims(input_shape_train))\n",
    "std_input_train = std(input_shape_train, dims=ndims(input_shape_train))\n",
    "\n",
    "input_data_train_normalized = (input_shape_train .- max_input_train) ./ (max_input_train - min_input_train)\n",
    "input_data_train_standardized = (input_shape_train .- mean_input_train) ./ std_input_train\n",
    "\n",
    "input_data_test_normalized = (input_shape_test .- max_input_train) ./ (max_input_train - min_input_train)\n",
    "input_data_test_standardized = (input_shape_test .- mean_input_train) ./ std_input_train;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output data, normalized and standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_output_train = maximum(output_shape_train)\n",
    "min_output_train = minimum(output_shape_train)\n",
    "\n",
    "mean_output_train = mean(output_shape_train, dims=ndims(output_shape_train))\n",
    "std_output_train = std(output_shape_train, dims=ndims(output_shape_train))\n",
    "\n",
    "output_data_train_normalized = (output_shape_train .- max_output_train) ./ (max_output_train - min_output_train)\n",
    "output_data_train_standardized = (output_shape_train .- mean_output_train) ./ std_output_train;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing different Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_function = NNlib.tanh\n",
    "\n",
    "models = [\n",
    "    Chain(\n",
    "    Dense(94 => 400, activation_function),               \n",
    "    Dropout(0.8),\n",
    "    Dense(400 => 47, identity),                     \n",
    "    ) \n",
    "] |> f64;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loss(flux_model, x, y)\n",
    "    ŷ = flux_model(x)\n",
    "    Flux.mse(ŷ, y, agg=sum)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux:params\n",
    "\n",
    "optimizer = Flux.AdaMax(0.001)\n",
    "loss_function = Flux.mse\n",
    "loss_discription = \"MSE\"\n",
    "\n",
    "epochs = 95\n",
    "batch_size = 64\n",
    "\n",
    "percentages_bc = [0.0, 0.01, 0.02, 0.05, 0.1, 0.12]\n",
    "n_configs_bc = Int.(4500 .* percentages_bc)\n",
    "\n",
    "loader = Flux.DataLoader((input_data_train_standardized, output_data_train_standardized), batchsize=batch_size, shuffle=true)\n",
    "\n",
    "for model in models\n",
    "    parameters = 0\n",
    "    layers = params(model)\n",
    "    for layer in layers\n",
    "        parameters += length(hcat(layer...))\n",
    "    end\n",
    "\n",
    "    outputDirectory = \"/Users/lukasgeyer/Studium/Computational Sciences/Masterarbeit/Tool Allesandro/repo/LmaPredict/benchmarks/ee+rr/$parameters\"\n",
    "\n",
    "    optim = Flux.setup(optimizer, model)\n",
    "    \n",
    "    function training()\n",
    "        losses = []\n",
    "        for epoch in 1:epochs\n",
    "            for (x, y) in loader\n",
    "                grads = gradient(m -> loss(m, x, y), model)\n",
    "                Flux.update!(optim, model, grads[1])\n",
    "                push!(losses,loss(model, x, y))\n",
    "            end\n",
    "        end\n",
    "        return losses\n",
    "    end\n",
    "\n",
    "    losses = training()\n",
    "    \n",
    "    out_of_sample_predictions = (model(input_data_test_standardized) .* std_output_train) .+ mean_output_train\n",
    "\n",
    "    analyse_predictions(\n",
    "        out_of_sample_predictions,\n",
    "        output_shape_test,\n",
    "        TSRC,\n",
    "        EIGVALS,\n",
    "        model,\n",
    "        optimizer,\n",
    "        loss_function,\n",
    "        loss_discription,\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        losses,\n",
    "        outputDirectory\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.2",
   "language": "julia",
   "name": "julia-1.10.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
