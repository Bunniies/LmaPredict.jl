{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different Neural Networks\n",
    "\n",
    "Each Network uses a vector of the eigen-eigen parts and the eigenvalues and computes the rest-eigen contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LmaPredict, Flux, Statistics, Plots, Random, StatsBase, JLD2, PrettyTables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"../plots\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const path_config = \"/Users/lukasgeyer/Studium/Computational Sciences/Masterarbeit/Daten Simon/dat\"\n",
    "const path_plot = \"../plots\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = readdir(path_config)[2:5001]\n",
    "idx = sortperm( parse.(Int64, fname))\n",
    "fname = fname[idx]\n",
    "em_n = \"VV\"\n",
    "\n",
    "cnfgarr = Vector{LMAConfig}(undef, 0)\n",
    "for f in fname\n",
    "    push!(cnfgarr, get_LMAConfig(joinpath(path_config, f), \"g5-g5\", em=em_n, bc=false))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data in training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCNFG = length(cnfgarr)\n",
    "train_size = 500\n",
    "test_size = NCNFG - train_size\n",
    "\n",
    "TSRC = \"24\"\n",
    "TVALS = length(cnfgarr[1].data[\"rr\"][TSRC]) - 1\n",
    "if em_n == \"PA\"\n",
    "    EIGVALS = 32\n",
    "else \n",
    "    EIGVALS = 64\n",
    "end\n",
    "\n",
    "eigvals_data_train = Array{Float32}(undef, EIGVALS, train_size)\n",
    "rr_data_train = Array{Float32}(undef, TVALS, train_size)\n",
    "ee_data_train = Array{Float32}(undef, TVALS, train_size)\n",
    "re_data_train = Array{Float32}(undef, TVALS, train_size)\n",
    "\n",
    "eigvals_data_test = Array{Float32}(undef, EIGVALS, test_size)\n",
    "rr_data_test = Array{Float64}(undef, TVALS, test_size)\n",
    "ee_data_test = Array{Float32}(undef, TVALS, test_size)\n",
    "re_data_test = Array{Float64}(undef, TVALS, test_size)\n",
    "\n",
    "for (k, dd) in enumerate(getfield.(cnfgarr, :data)[1:train_size])\n",
    "    eigvals_data_train[:,k] = copy(cnfgarr[k].data[\"eigvals\"][1:EIGVALS])\n",
    "    rr_data_train[:,k] = getindex(getindex(dd, \"rr\"), TSRC)[2:end]\n",
    "    ee_data_train[:,k] = getindex(getindex(dd, \"ee\"), TSRC)[2:end]\n",
    "    re_data_train[:,k] = getindex(getindex(dd, \"re\"), TSRC)[2:end]\n",
    "end\n",
    "for (k, dd) in enumerate(getfield.(cnfgarr, :data)[train_size+1:NCNFG])\n",
    "    eigvals_data_test[:,k] = copy(cnfgarr[k].data[\"eigvals\"][1:EIGVALS])\n",
    "    rr_data_test[:,k] = getindex(getindex(dd, \"rr\"), TSRC)[2:end]\n",
    "    ee_data_test[:,k] = getindex(getindex(dd, \"ee\"), TSRC)[2:end]\n",
    "    re_data_test[:,k] = getindex(getindex(dd, \"re\"), TSRC)[2:end]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = TVALS + EIGVALS\n",
    "output_length = TVALS\n",
    "\n",
    "input_shape_train = vcat(1 ./ eigvals_data_train, ee_data_train)\n",
    "output_shape_train = re_data_train\n",
    "\n",
    "input_shape_test = vcat(1 ./ eigvals_data_test, ee_data_test)\n",
    "output_shape_test = re_data_test;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_train = maximum(input_shape_train)\n",
    "min_input_train = minimum(input_shape_train)\n",
    "\n",
    "mean_input_train = mean(mean.([input_shape_train[:,i] for i in 1:train_size]))\n",
    "std_input_train = std(mean.([input_shape_train[:,i] for i in 1:train_size]))\n",
    "\n",
    "input_data_train_normalized = (input_shape_train .- max_input_train) ./ (max_input_train - min_input_train)\n",
    "input_data_train_standardized = (input_shape_train .- mean_input_train) ./ std_input_train\n",
    "\n",
    "input_data_test_normalized = (input_shape_test .- max_input_train) ./ (max_input_train - min_input_train)\n",
    "input_data_test_standardized = (input_shape_test .- mean_input_train) ./ std_input_train;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_output_train = maximum(output_shape_train)\n",
    "min_output_train = minimum(output_shape_train)\n",
    "\n",
    "mean_output_train = mean(mean.([output_shape_train[:,i] for i in 1:train_size]))\n",
    "std_output_train = std(mean.([output_shape_train[:,i] for i in 1:train_size]))\n",
    "\n",
    "output_data_train_normalized = (output_shape_train .- max_output_train) ./ (max_output_train - min_output_train)\n",
    "output_data_train_standardized = (output_shape_train .- mean_output_train) ./ std_output_train;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing different Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = [NNlib.tanh, NNlib.celu, NNlib.elu, NNlib.tanhshrink];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "for activation_function in activation_functions\n",
    "    push!(models,\n",
    "        [\n",
    "    Chain(\n",
    "    Dense(input_length => 1, activation_function),\n",
    "    Dense(1 => output_length, activation_function),\n",
    "    ),\n",
    "    Chain(\n",
    "    Dense(input_length => 5, activation_function),\n",
    "    Dense(5 => output_length, activation_function),\n",
    "    ),\n",
    "    Chain(\n",
    "    Dense(input_length => 10, activation_function),\n",
    "    Dense(10 => output_length, activation_function),\n",
    "    ),\n",
    "    Chain(\n",
    "    Dense(input_length => 50, activation_function),\n",
    "    Dense(50 => output_length, activation_function),\n",
    "    ),\n",
    "    Chain(\n",
    "    Dense(input_length => 5, activation_function),\n",
    "    Dense(5 => 1, activation_function),\n",
    "    Dense(1 => output_length, activation_function)\n",
    "    ),\n",
    "    Chain(\n",
    "    Dense(input_length => 10, activation_function),\n",
    "    Dense(10 => 1, activation_function),\n",
    "    Dense(1 => output_length, activation_function),\n",
    "    ),\n",
    "    Chain(\n",
    "    Dense(input_length => 50, activation_function),\n",
    "    Dense(50 => 1, activation_function),\n",
    "    Dense(1 => output_length, activation_function),\n",
    "    ),\n",
    "    Chain(\n",
    "    Dense(input_length => 10, activation_function),\n",
    "    Dense(10 => 5, activation_function),\n",
    "    Dense(5 => output_length, activation_function),\n",
    "    ),\n",
    "    Chain(\n",
    "    Dense(input_length => 50, activation_function),\n",
    "    Dense(50 => 5, activation_function),\n",
    "    Dense(5 => output_length, activation_function),\n",
    "    ),\n",
    "    Chain(\n",
    "    Dense(input_length => 50, activation_function),\n",
    "    Dense(50 => 5, activation_function),\n",
    "    Dense(5 => 1, activation_function),\n",
    "    Dense(1 => output_length, activation_function),\n",
    "    ),\n",
    "    Chain(\n",
    "    Dense(input_length => 50, activation_function),\n",
    "    Dense(50 => 50, activation_function),\n",
    "    Dense(50 => output_length, activation_function),\n",
    "    ),\n",
    "    Chain(\n",
    "    Dense(input_length => 2*input_length, activation_function),\n",
    "    Dense(2*input_length => 2*input_length, activation_function),\n",
    "    Dense(2*input_length => output_length, activation_function),\n",
    "    ),\n",
    "    Chain(\n",
    "    Dense(input_length => 2*input_length, activation_function),\n",
    "    Dense(2*input_length => input_length, activation_function),\n",
    "    Dense(input_length => output_length, activation_function),\n",
    "    Dense(output_length => output_length, activation_function),\n",
    "    ),\n",
    "    Chain(\n",
    "    Dense(input_length => 2*input_length, activation_function),\n",
    "    Dense(2*input_length => 2*input_length, activation_function),\n",
    "    Dense(2*input_length => input_length, activation_function),\n",
    "    Dense(input_length => output_length, activation_function),\n",
    "    ),\n",
    "    Chain(\n",
    "    Dense(input_length => 2*input_length, activation_function),\n",
    "    Dense(2*input_length => 2*input_length, activation_function),\n",
    "    Dense(2*input_length => 2*input_length, activation_function),\n",
    "    Dense(2*input_length => output_length, activation_function),\n",
    "    )\n",
    "     ])\n",
    "end\n",
    "\n",
    "models = vcat(models...);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing parameter regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.1, 0.01, 0.001];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = []\n",
    "push!(optimizers,[Flux.AdaGrad(), Flux.AdaDelta(), Flux.AMSGrad(), Flux.NAdam()])\n",
    "for learning_rate in learning_rates\n",
    "    push!(optimizers,\n",
    "        [\n",
    "            Flux.Adam(learning_rate),\n",
    "            Flux.RAdam(learning_rate),\n",
    "            Flux.AdaMax(learning_rate),\n",
    "            Flux.AdamW(learning_rate),\n",
    "            Flux.OAdam(learning_rate)\n",
    "            ])\n",
    "end\n",
    "optimizers = vcat(optimizers...);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_functions = [Flux.mse, Flux.mae]\n",
    "\n",
    "epochs = [1_000, 10_000, 50_000, 100_000, 200_000]\n",
    "\n",
    "batch_sizes = [32, 64];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perfoming Grid-Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_scores = length(models) * length(loss_functions) * length(epochs) * length(batch_sizes) * length(optimizers)\n",
    "\n",
    "scores_table_header = [\"Index\", \"|σ_p - σ_t|\", \"max(|μ_p - μ_t| / μ_t)\"]\n",
    "lookup_table_header = [\"Index\", \"Model\", \"Optimizer\", \"Loss Function\", \"Epochs\", \"Batch Size\"]\n",
    "\n",
    "scores_mat = Matrix{Float64}(undef, n_scores, length(scores_table_header))\n",
    "lookup_mat = Matrix{Any}(undef, n_scores, length(lookup_table_header))\n",
    "\n",
    "index = 1\n",
    "\n",
    "for model in models\n",
    "    for optimizer in optimizers\n",
    "        for loss_function in loss_functions\n",
    "            for epoch in epochs\n",
    "                for batch_size in batch_sizes\n",
    "                    \n",
    "                    loader = Flux.DataLoader(\n",
    "                        (input_data_train_normalized, output_data_train_normalized),\n",
    "                        batchsize=batch_size,\n",
    "                        shuffle=true)\n",
    "\n",
    "                    optim = Flux.setup(optimizer, model)\n",
    "\n",
    "                    function training()\n",
    "                        for e in 1:epoch\n",
    "                            for (x,y) in loader\n",
    "                                _, grads = Flux.withgradient(model) do m\n",
    "                                    y_hat = m(x)\n",
    "                                    loss_function(y_hat, y)\n",
    "                                end\n",
    "                                Flux.update!(optim, model, grads[1])\n",
    "                            end\n",
    "                        end\n",
    "                    end\n",
    "\n",
    "                    training()\n",
    "\n",
    "                    out_of_sample_predictions = (model(input_data_test_normalized) .* (max_output_train - min_output_train)) .+ max_output_train\n",
    "                    out_of_sample_error = [loss_function(out_of_sample_predictions[:,i], output_shape_test[:,i]) for i in 1:test_size]\n",
    "\n",
    "                    mean_target = mean.([output_shape_test[i,:] for i in 1:TVALS])\n",
    "                    σ_target = std.([output_shape_test[i,:] for i in 1:TVALS]) ./ sqrt(test_size - 1)\n",
    "        \n",
    "                    mean_predicted = mean.([out_of_sample_predictions[i,:] for i in 1:TVALS])\n",
    "                    σ_predicted = std.([out_of_sample_predictions[i,:] for i in 1:TVALS]) ./ sqrt(test_size - 1)\n",
    "\n",
    "                    mean_Δσ = mean(abs.(σ_target - σ_predicted))\n",
    "                    max_Δμ = maximum(abs.((mean_target - mean_predicted) ./ mean_target))\n",
    "\n",
    "                    scores_mat[index,:] = [index, mean_Δσ, max_Δμ]\n",
    "\n",
    "                    lookup_mat[index,:] = [index, model, optimizer, loss_function, epoch, batch_size]\n",
    "\n",
    "                    Flux.loadparams!(model,map(p -> p .= randn.(), Flux.params(model)))\n",
    "                    index += 1\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../gridSearch/ee+ev/\"\n",
    "\n",
    "scores_file = output_dir * \"scores.txt\"\n",
    "scores_file_mean = output_dir * \"scores_meanSorted.txt\"\n",
    "lookup_file = output_dir * \"lookup.txt\"\n",
    "\n",
    "mkpath(output_dir)\n",
    "rm(scores_file, force=true)\n",
    "rm(scores_file_mean, force=true)\n",
    "rm(lookup_file, force=true)\n",
    "\n",
    "open(scores_file, \"a\") do file\n",
    "    pretty_table(file, scores_mat[sortperm(scores_mat[:, 2]), :], header=scores_table_header)\n",
    "end\n",
    "\n",
    "open(scores_file_mean, \"a\") do file\n",
    "    pretty_table(file, scores_mat[sortperm(scores_mat[:, 3]), :], header=scores_table_header)\n",
    "end\n",
    "\n",
    "open(lookup_file, \"a\") do file\n",
    "    pretty_table(file, lookup_mat, header=lookup_table_header)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = 2967\n",
    "index = 2609\n",
    "\n",
    "optimizer = lookup_mat[index,3]\n",
    "loss_function = lookup_mat[index,4]\n",
    "epochs = lookup_mat[index, 5]\n",
    "batch_size = lookup_mat[index, 6]\n",
    "\n",
    "model = lookup_mat[index,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.loadparams!(model,map(p -> p .= randn.(), Flux.params(model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Flux.DataLoader((input_data_train_normalized, output_data_train_normalized), batchsize=batch_size, shuffle=true)\n",
    "optim = Flux.setup(optimizer, model)\n",
    "\n",
    "function training()\n",
    "    losses = []\n",
    "    for epoch in 1:epochs\n",
    "        for (x, y) in loader\n",
    "            loss, grads = Flux.withgradient(model) do m\n",
    "                y_hat = m(x)\n",
    "                loss_function(y_hat,y)\n",
    "            end\n",
    "            Flux.update!(optim, model, grads[1])\n",
    "            push!(losses, loss) \n",
    "        end\n",
    "    end\n",
    "        \n",
    "    return losses\n",
    "end\n",
    "\n",
    "losses = training()\n",
    "\n",
    "out_of_sample_predictions = (model(input_data_test_normalized) .* (max_output_train - min_output_train)) .+ max_output_train\n",
    "out_of_sample_error = [loss_function(out_of_sample_predictions[:,i], output_shape_test[:,i]) for i in 1:test_size];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = @layout [a b c; d e f; g h i]\n",
    "\n",
    "c1 = rand([i for i in 1:500])\n",
    "p1 = scatter(output_shape_test[:,c1], label=\"Actual\")\n",
    "scatter!(p1, out_of_sample_predictions[:,c1], label=\"Prediction\", legend=:top)\n",
    "\n",
    "c2 = rand([i for i in 1:500])\n",
    "p2 = scatter(output_shape_test[:,c2], label=\"Actual\")\n",
    "scatter!(p2, out_of_sample_predictions[:,c2], label=\"Prediction\", legend=:top)\n",
    "\n",
    "c3 = rand([i for i in 1:500])\n",
    "p3 = scatter(output_shape_test[:,c3], label=\"Actual\")\n",
    "scatter!(p3, out_of_sample_predictions[:,c3], label=\"Prediction\", legend=:top, )\n",
    "\n",
    "c4 = rand([i for i in 1:500])\n",
    "p4 = scatter(output_shape_test[:,c4], label=\"Actual\")\n",
    "scatter!(p4, out_of_sample_predictions[:,c4], label=\"Prediction\", legend=:top)\n",
    "\n",
    "c5 = rand([i for i in 1:500])\n",
    "p5 = scatter(output_shape_test[:,c5], label=\"Actual\")\n",
    "scatter!(p5, out_of_sample_predictions[:,c5], label=\"Prediction\", legend=:top)\n",
    "\n",
    "c6 = rand([i for i in 1:500])\n",
    "p6 = scatter(output_shape_test[:,c6], label=\"Actual\")\n",
    "scatter!(p6, out_of_sample_predictions[:,c6], label=\"Prediction\", legend=:top)\n",
    "\n",
    "c7 = rand([i for i in 1:500])\n",
    "p7 = scatter(output_shape_test[:,c7], label=\"Actual\")\n",
    "scatter!(p7, out_of_sample_predictions[:,c7], label=\"Prediction\", legend=:top)\n",
    "\n",
    "c8 = rand([i for i in 1:500])\n",
    "p8 = scatter(output_shape_test[:,c8], label=\"Actual\")\n",
    "scatter!(p8, out_of_sample_predictions[:,c8], label=\"Prediction\", legend=:top)\n",
    "\n",
    "c9 = rand([i for i in 1:500])\n",
    "p9 = scatter(output_shape_test[:,c9], label=\"Actual\")\n",
    "scatter!(p9, out_of_sample_predictions[:,c9], label=\"Prediction\", legend=:top)\n",
    "\n",
    "plot(p1, p2, p3, p4, p5, p6, p7, p8, p9, layout = l, size=(1200,1000), dpi=1000, markerstrokewidth = 0)\n",
    "#savefig(joinpath(path_plot, \"neural_network_test.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_target = mean.([output_shape_test[i,:] for i in 1:TVALS])\n",
    "σ_target = std.([output_shape_test[i,:] for i in 1:TVALS]) ./ sqrt(test_size - 1)\n",
    "        \n",
    "mean_predicted = mean.([out_of_sample_predictions[i,:] for i in 1:TVALS])\n",
    "σ_predicted = std.([out_of_sample_predictions[i,:] for i in 1:TVALS]) ./ sqrt(test_size - 1)\n",
    "\n",
    "max_Δμ = maximum(abs.((mean_target - mean_predicted) ./ mean_target))\n",
    "\n",
    "p = scatter(\n",
    "        size=(1400,1000),\n",
    "        dpi = 1000,\n",
    "        thickness_scaling = 1.5,\n",
    ")\n",
    "    \n",
    "scatter!(p,\n",
    "    mean_target[7:41],\n",
    "    yerr=σ_target[7:41],\n",
    "    label=\"actual\",\n",
    "    legend=:bottom,\n",
    "    linecolor=:blue,\n",
    "    marker=:xcross,\n",
    "    markersize = 5,\n",
    "    markerstrokewidth = 0.3\n",
    ")\n",
    "scatter!(p,\n",
    "    mean_predicted[7:41],\n",
    "    yerr=σ_predicted[7:41],\n",
    "    label=\"predicted\",\n",
    "    legend=:bottom,\n",
    "    linecolor=:red,\n",
    "    marker =:+,\n",
    "    markersize = 5,\n",
    "    markerstrokewidth = 0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
